# -*- coding: utf-8 -*-
"""Text_Summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qm9CsWuxwtpiiDMrKHPh7zl1ejZN0-Ft

**EXTRACTIVE SUMMARIZATION**
"""

!pip install Pypdf
! pip install rouge

import pypdf
from pypdf import PdfReader
import os

def extract_text_from_pdf(pdf_path):
    text = ""
    reader = PdfReader(pdf_path)
    num_pages = len(reader.pages)
    for page_num in range(num_pages):
            page = reader.pages[page_num]
            text += page.extract_text()
    return text
pdf_path='/content/drive/MyDrive/GLM/budget_speech (1).pdf'
extracted_text = extract_text_from_pdf(pdf_path)
print(extracted_text[800:1000])

"""TEXT PREPROCESSING"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.corpus import stopwords
from collections import Counter

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

#lower case conversion
text=extracted_text.lower()
data=text[801:36100]

# special charecter removal
def clean_text(sentences):
    cleaned_sentences = []
    for sentence in sentences:
        # Remove newlines
        sentence = sentence.replace('\n', ' ')

        # Remove serial numbers (e.g., "1. ", "2. ", etc.)
        words = sentence.split()
        cleaned_words = []
        for word in words:
            if  word[:-1].isdigit() and len(word) < 4:
                continue
            cleaned_words.append(word)
        cleaned_sentence = ' '.join(cleaned_words)
        cleaned_sentences.append(cleaned_sentence)

    return cleaned_sentences

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from heapq import nlargest
# Summary generator
def generate_summary(text, n):
  # sentence tokenization
  sentences = sent_tokenize(text)
  # Punctuation and special charecters removal
  cleaned_text = clean_text(sentences)

  # Create the TF-IDF matrix
  vectorizer = TfidfVectorizer(stop_words='english')
  tfidf_matrix = vectorizer.fit_transform(cleaned_text)

  # cosine similarity between each sentence and the document
  sentence_scores = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])[0]

  # Select the top n sentences with the highest scores
  summary_sentences = nlargest(n, range(len(sentence_scores)), key=sentence_scores.__getitem__)

  summary_tfidf = ' '.join([cleaned_text[i] for i in sorted(summary_sentences)])

  return summary_tfidf

summary = generate_summary(data, 20)
summary_sentences = summary.split('. ')
formatted_summary = '.\n'.join(summary_sentences)

print(formatted_summary)

reference_summary="The Indian government, led by Prime Minister Shri Narendra Modi, aims to make India a viksit bharat by 2047 by improving people's capabilities and empowering them. Social justice is an effective governance model, and the government has implemented various initiatives to achieve this goal. These include direct benefit transfers, crop insurance, and mudra yojana loans. The government has a strong track record of governance, development, and performance, providing transparent, accountable, and people-centric administration. The vision for 'viksit bharat' is a prosperous bharat in harmony with nature, modern infrastructure, and providing opportunities for all citizens and regions to reach their potential. The government is also working on reforms in the states to achieve this vision. A committee will be mandated to make recommendations for addressing challenges comprehensively."

# Evaluation
import rouge
from rouge import Rouge
def evaluate_rouge(reference_text, summary_text):
  rouge = Rouge()
  scores = rouge.get_scores(reference_text, summary_text)
  return scores[0]['rouge-1']['f']

summary = generate_summary(data, 20)

# Evaluate the summary using ROUGE
rouge_score = evaluate_rouge(reference_summary, summary)

print(f"ROUGE score: {rouge_score}")

"""**ABSTRACTIVE SUMMARIZATION**"""

from transformers import T5Tokenizer, T5ForConditionalGeneration

model=T5ForConditionalGeneration.from_pretrained('t5-base')
tokenizer=T5Tokenizer.from_pretrained('t5-base')

text1=data[801:4000]

input=tokenizer.encode(text1,return_tensors='pt',max_length=512,truncation=True)

input=tokenizer.encode(data,return_tensors='pt',max_length=512,truncation=True)

output=model.generate(input,max_length=150,num_beams=7,early_stopping=True)

output_text=tokenizer.decode(output[0],skip_special_tokens=True)
print(output_text)

summary = generate_summary(data, 20)

# Evaluate the summary using ROUGE
rouge_score = evaluate_rouge(output_text, summary)

print(f"ROUGE score: {rouge_score}")